{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file to get data\n",
    "file1 = open(\"train.csv\")\n",
    "file2 = open(\"test.csv\")\n",
    "\n",
    "csvreader1 = csv.reader(file1)\n",
    "csvreader2 = csv.reader(file2)\n",
    "\n",
    "header_train = []\n",
    "header_train = next(csvreader1)\n",
    "\n",
    "next(csvreader2)\n",
    "\n",
    "data_train = []\n",
    "for row in csvreader1:\n",
    "    data_train.append(row)\n",
    "\n",
    "data_test = []\n",
    "for row in csvreader2:\n",
    "    data_test.append(row)\n",
    "\n",
    "file1.close()\n",
    "file2.close()\n",
    "\n",
    "    # Turn our data in np.array and remove the first attribue which is just numeration\n",
    "data_train = np.array(data_train, dtype=float)\n",
    "data_train = np.delete(data_train, 0, axis=1)\n",
    "\n",
    "data_test = np.array(data_test, dtype=float)\n",
    "data_test = np.delete(data_test, 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMaxLikelihood:\n",
    "    def __init__(self, n_dims):\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros(n_dims)\n",
    "        # Nous avons un scalaire comme écart-type car notre modèle est une loi gaussienne isotropique\n",
    "        self.sigma_sq = 1.0\n",
    "\n",
    "    # Pour un jeu d'entraînement, la fonction devrait calculer les estimateur ML de l'espérance et de la variance\n",
    "    def train(self, train_data):\n",
    "        # Ici, nous devons trouver la moyenne et la variance dans train_data et les définir dans self.mu and self.\n",
    "\n",
    "        self.mu = np.mean(train_data, axis=0)\n",
    "        self.sigma_sq = np.sum((train_data - self.mu) ** 2.0) / (self.n_dims * train_data.shape[0])\n",
    "\n",
    "    # Retourne un vecteur de dimension égale au nombre d'ex. test qui contient les log probabilité de chaque\n",
    "    # exemple test\n",
    "    def loglikelihood(self, test_data):\n",
    "        # la ligne suivante calcule le log(normalization constant) i.e. log( 1/(sigma sqrt(2pi)) )\n",
    "        c = self.n_dims * -(np.log(np.sqrt(self.sigma_sq)) + (1 / 2) * np.log(2 * np.pi))\n",
    "        # Il est nécessaire de calculer la log-probabilité de chaque exemple test sous le modèle déterminé\n",
    "        # par mu et sigma_sq. Le vecteur de probabilité est/sera log_prob\n",
    "\n",
    "        log_prob = c - np.sum((test_data - self.mu) ** 2.0, axis=1) / (2.0 * self.sigma_sq)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, maximum_likelihood_models, priors):\n",
    "        self.maximum_likelihood_models = maximum_likelihood_models\n",
    "        self.priors = priors\n",
    "        if len(self.maximum_likelihood_models) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.maximum_likelihood_models)\n",
    "\n",
    "    # Retourne une matrice de dimension [nb d'ex. test, nb de classes] contenant les log\n",
    "    # probabilités de chaque ex. test sous le modèle entrainé par le MV.\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        log_pred = np.zeros((test_data.shape[0], self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # Ici, nous devrons utiliser maximum_likelihood_models[i] et priors pour remplir\n",
    "            # chaque colonne de log_pred (c'est plus efficace de remplir une colonne à la fois)\n",
    "\n",
    "            log_pred[:, i] = self.maximum_likelihood_models[i].loglikelihood(\n",
    "                test_data) + np.log(self.priors[i])\n",
    "\n",
    "        return log_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data_train[:,0:20]\n",
    "Y_train=data_train[:,-1]\n",
    "\n",
    "X_test = data_test[:, 0:20]\n",
    "Y_test = data_test[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train[0]))\n",
    "print(data_train[0][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_trainClass0=data_train[data_train[:,-1]==0]\n",
    "Data_trainClass1 = data_train[data_train[:, -1] == 1]\n",
    "Data_trainClass2 = data_train[data_train[:, -1] == 2]\n",
    "\n",
    "model_class1 = GaussianMaxLikelihood(19)\n",
    "model_class2 = GaussianMaxLikelihood(19)\n",
    "model_class3 = GaussianMaxLikelihood(19)\n",
    "model_class1.train(Data_trainClass0)\n",
    "model_class2.train(Data_trainClass0)\n",
    "model_class3.train(Data_trainClass0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.05345502e+01  2.30312500e+02  3.65526810e+01 ...  6.53855057e+01\n",
      "   2.00107200e+07  0.00000000e+00]\n",
      " [-2.05345502e+01  2.30000000e+02  3.63707161e+01 ...  6.53918381e+01\n",
      "   2.00107200e+07  0.00000000e+00]\n",
      " [-2.05345502e+01  2.29687500e+02  3.59717674e+01 ...  6.54068909e+01\n",
      "   2.00107200e+07  0.00000000e+00]\n",
      " ...\n",
      " [ 1.32594524e+01  2.53750000e+02  5.77351799e+01 ...  6.64838333e+01\n",
      "   2.00112070e+07  0.00000000e+00]\n",
      " [ 1.34941330e+01  2.53750000e+02  5.84715576e+01 ...  6.64385223e+01\n",
      "   2.00112070e+07  0.00000000e+00]\n",
      " [ 1.34941330e+01  2.53750000e+02  5.84715576e+01 ...  6.64385223e+01\n",
      "   2.00112070e+07  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Data_trainClass0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ml = [model_class1, model_class2, model_class3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Priors:\n",
      "Class 0 Prior: 0.7859472743521001\n",
      "Class 1 Prior: 0.04077301161751564\n",
      "Class 2 Prior: 0.17327971403038428\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class priors\n",
    "total_samples = data_train.shape[0]\n",
    "\n",
    "# Count the number of samples in each class\n",
    "num_samples_class0 = len(Data_trainClass0)\n",
    "num_samples_class1 = len(Data_trainClass1)\n",
    "num_samples_class2 = len(Data_trainClass2)\n",
    "\n",
    "# Calculate the class priors\n",
    "prior_class0 = num_samples_class0 / total_samples\n",
    "prior_class1 = num_samples_class1 / total_samples\n",
    "prior_class2 = num_samples_class2 / total_samples\n",
    "\n",
    "print(\"Class Priors:\")\n",
    "print(\"Class 0 Prior:\", prior_class0)\n",
    "print(\"Class 1 Prior:\", prior_class1)\n",
    "print(\"Class 2 Prior:\", prior_class2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors=np.array([prior_class0,prior_class1,prior_class2])\n",
    "classifier = BayesClassifier(model_ml, priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, labels):\n",
    "    # Nous pouvons calculez les log-probabilités selon notre modèle\n",
    "    log_prob = classifier.loglikelihood(data)\n",
    "    # Il reste à calculer les classes prédites\n",
    "    classes_pred = log_prob.argmax(1) \n",
    "    # Retournez l'exactitude en comparant les classes prédites aux vraies étiquettes\n",
    "    acc = np.mean(classes_pred == labels)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is : 78.6 % \n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is : {:.1f} % \".format(\n",
    "    100 * get_accuracy(X_train, Y_train)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
