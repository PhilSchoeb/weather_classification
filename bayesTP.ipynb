{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file to get data\n",
    "file1 = open(\"train.csv\")\n",
    "file2 = open(\"test.csv\")\n",
    "\n",
    "csvreader1 = csv.reader(file1)\n",
    "csvreader2 = csv.reader(file2)\n",
    "\n",
    "header_train = []\n",
    "header_train = next(csvreader1)\n",
    "\n",
    "next(csvreader2)\n",
    "\n",
    "data_train = []\n",
    "for row in csvreader1:\n",
    "    data_train.append(row)\n",
    "\n",
    "data_test = []\n",
    "for row in csvreader2:\n",
    "    data_test.append(row)\n",
    "\n",
    "file1.close()\n",
    "file2.close()\n",
    "\n",
    "    # Turn our data in np.array and remove the first attribue which is just numeration\n",
    "data_train = np.array(data_train, dtype=float)\n",
    "data_train = np.delete(data_train, 0, axis=1)\n",
    "\n",
    "data_test = np.array(data_test, dtype=float)\n",
    "data_test = np.delete(data_test, 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMaxLikelihood:\n",
    "    def __init__(self, n_dims, cov_type='isotropic'):\n",
    "        self.cov_type = cov_type\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros(n_dims)\n",
    "        # Nous avons un scalaire comme écart-type car notre modèle est une loi gaussienne isotropique\n",
    "        self.sigma_sq = 1.0\n",
    "\n",
    "    # Pour un jeu d'entraînement, la fonction devrait calculer les estimateur ML de l'espérance et de la variance\n",
    "    def train(self, train_data):\n",
    "        # Ici, nous devons trouver la moyenne et la variance dans train_data et les définir dans self.mu and self.\n",
    "\n",
    "        self.mu = np.mean(train_data, axis=0)\n",
    "\n",
    "        # here we will create t\bhe covariance matrix\n",
    "        if self.cov_type == 'isotropic':\n",
    "            # Identity times sigma square\n",
    "            self.covariance = np.eye(\n",
    "                self.n_dims) * np.sum((train_data - self.mu) ** 2.0) / (self.n_dims * train_data.shape[0])\n",
    "        elif self.cov_type == 'diagonal':\n",
    "            # put the variance on the diagonal\n",
    "            self.covariance = np.diag(np.var(train_data, axis=0))\n",
    "        else:\n",
    "            # Calculate the full covariance matrix\n",
    "            self.covariance = np.cov(train_data, rowvar=False)\n",
    "\n",
    "    # Retourne un vecteur de dimension égale au nombre d'ex. test qui contient les log probabilité de chaque\n",
    "    # exemple test\n",
    "\n",
    "    def loglikelihood(self, test_data):\n",
    "        # Calculer la constante de normalisation de la façon standard, sans raccourci\n",
    "        c = -(np.log(np.sqrt(np.linalg.det(self.covariance))) +\n",
    "              (self.n_dims / 2) * np.log(2 * np.pi))\n",
    "        # Ensuite la log prob\n",
    "        # Notez l'absence d'un second np.dot. Pouvez-vous deviner pourquoi?\n",
    "        log_prob = c - (np.dot((test_data - self.mu), np.linalg.inv(self.covariance))\n",
    "                        * (test_data - self.mu)).sum(axis=1) / 2\n",
    "        return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, maximum_likelihood_models, priors):\n",
    "        self.maximum_likelihood_models = maximum_likelihood_models\n",
    "        self.priors = priors\n",
    "        if len(self.maximum_likelihood_models) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.maximum_likelihood_models)\n",
    "\n",
    "    # Retourne une matrice de dimension [nb d'ex. test, nb de classes] contenant les log\n",
    "    # probabilités de chaque ex. test sous le modèle entrainé par le MV.\n",
    "    def loglikelihood(self, test_data):\n",
    "\n",
    "        log_pred = np.zeros((test_data.shape[0], self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # Ici, nous devrons utiliser maximum_likelihood_models[i] et priors pour remplir\n",
    "            # chaque colonne de log_pred (c'est plus efficace de remplir une colonne à la fois)\n",
    "\n",
    "            log_pred[:, i] = self.maximum_likelihood_models[i].loglikelihood(\n",
    "                test_data) + np.log(self.priors[i])\n",
    "\n",
    "        return log_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train=data_train[:,0:19]\n",
    "#Y_train=data_train[:,-1]\n",
    "\n",
    "X_test = data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_test[0]))\n",
    "#print(len(data_test[0]))\n",
    "#print(len(X_train[0]))\n",
    "#print(len(data_train[0]))\n",
    "\n",
    "#print(len(X_test))\n",
    "#print(len(data_test))\n",
    "#print(len(X_train))\n",
    "#print(len(data_train))\n",
    "\n",
    "#print(len(X_train[0]))\n",
    "#print(len(X_val))\n",
    "#print(len(Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(data_train[0]))\n",
    "#print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35179\n",
      "35179\n",
      "35179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "strategy = {0: 35179, 1: 35179, 2:35179}\n",
    "oversample = SMOTE(sampling_strategy=strategy)\n",
    "X, y = oversample.fit_resample(data_train[:, 0:19], data_train[:, -1])\n",
    "\n",
    "\n",
    "print(np.sum(np.hstack((X,y.reshape(-1,1)))[:,-1]==0))\n",
    "print(np.sum(np.hstack((X,y.reshape(-1,1)))[:,-1]==1))\n",
    "print(np.sum(np.hstack((X, y.reshape(-1, 1)))[:, -1] == 2))\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "DataTrainClass = np.hstack((X_train, Y_train.reshape(-1, 1)))#split data and use X_Train and YTrain to create classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data_trainClass0=DataTrainClass [DataTrainClass [:,-1]==0]\n",
    "Data_trainClass1 = DataTrainClass [DataTrainClass [:, -1] == 1]\n",
    "Data_trainClass2 = DataTrainClass[DataTrainClass[:, -1] == 2]\n",
    "\n",
    "cov_type = 'full'\n",
    "#cov_type = 'isotropic'\n",
    "#cov_type = 'diagonal'\n",
    "\n",
    "\n",
    "model_class1 = GaussianMaxLikelihood(19,cov_type)\n",
    "model_class2 = GaussianMaxLikelihood(19,cov_type)\n",
    "model_class3 = GaussianMaxLikelihood(19, cov_type)\n",
    "model_class1.train(Data_trainClass0[:,0:19])\n",
    "model_class2.train(Data_trainClass1[:,0:19])\n",
    "model_class3.train(Data_trainClass2[:,0:19])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Data_trainClass0[:, 0:19])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ml = [model_class1, model_class2, model_class3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Priors:\n",
      "Class 0 Prior: 0.33231472598277845\n",
      "Class 1 Prior: 0.33373603856494805\n",
      "Class 2 Prior: 0.3339492354522735\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class priors\n",
    "total_samples = DataTrainClass.shape[0]\n",
    "\n",
    "# Count the number of samples in each class\n",
    "num_samples_class0 = len(Data_trainClass0)\n",
    "num_samples_class1 = len(Data_trainClass1)\n",
    "num_samples_class2 = len(Data_trainClass2)\n",
    "\n",
    "# Calculate the class priors\n",
    "prior_class0 = num_samples_class0 / total_samples\n",
    "prior_class1 = num_samples_class1 / total_samples\n",
    "prior_class2 = num_samples_class2 / total_samples\n",
    "\n",
    "print(\"Class Priors:\")\n",
    "print(\"Class 0 Prior:\", prior_class0)\n",
    "print(\"Class 1 Prior:\", prior_class1)\n",
    "print(\"Class 2 Prior:\", prior_class2)\n",
    "print(prior_class0+prior_class1+prior_class2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors=np.array([prior_class0,prior_class1,prior_class2])\n",
    "classifier = BayesClassifier(model_ml, priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, labels):\n",
    "    # Nous pouvons calculez les log-probabilités selon notre modèle\n",
    "    log_prob = classifier.loglikelihood(data)\n",
    "    # Il reste à calculer les classes prédites\n",
    "    classes_pred = log_prob.argmax(1)\n",
    "    # Retournez l'exactitude en comparant les classes prédites aux vraies étiquettes\n",
    "    acc = np.mean(classes_pred == labels)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is : 79.0 % \n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is : {:.1f} % \".format(\n",
    "    100 * get_accuracy(X_val, Y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "log_prob = classifier.loglikelihood(X_test)\n",
    "# Il reste à calculer les classes prédites\n",
    "classes_pred = log_prob.argmax(1)\n",
    "\n",
    "print(classes_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'output.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['SNo', 'Label'])  # Write the header\n",
    "    for sno, label in enumerate(classes_pred, start=1):\n",
    "        writer.writerow([sno, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
